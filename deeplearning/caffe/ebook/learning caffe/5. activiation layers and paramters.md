# 激活层（Activiation Layers)及参数

 在激活层中，对输入数据进行激活操作（实际上就是一种函数变换），是逐元素进行运算的。从bottom得到一个blob数据输入，运算后，从top输入一个blob数据。在运算过程中，没有改变数据的大小，即输入和输出的数据大小是相等的。

输入：`n*c*h*w`
输出：`n*c*h*w`
常用的激活函数有`sigmoid`, `tanh`,`relu`等，下面分别介绍。

1.`Sigmoid`
 对每个输入数据，利用`sigmoid`函数执行操作。这种层设置比较简单，没有额外的参数。
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
$$S(x) = \frac{1}{1+e^{-x}}$$

层类型：`Sigmoid`
**示例`1`**：
```
layer {
  name: "encode1neuron"
  bottom: "encode1"
  top: "encode1neuron"
  type: "Sigmoid"
}
```

2.`ReLU / Rectified-Linear and Leaky-ReLU`
`ReLU`是目前使用最多的激活函数，主要因为其收敛更快，并且能保持同样效果。
标准的`ReLU`函数为`max(x, 0)`，当`x>0`时，输出`x`; 当`x<=0`时，输出`0`
```
f(x)=max(x,0)
```
层类型：`ReLU`
(1) 可选参数：
 `negative_slope`：默认为`0`. 对标准的`ReLU`函数进行变化，如果设置了这个值，那么数据为负数时，就不再设置为0，而是用原始数据乘以`negative_slope`
```
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
```
`RELU`层支持`in-place`计算，这意味着`bottom`的输出和输入相同以避免内存的消耗。

3.`TanH / Hyperbolic Tangent`
利用双曲正切函数对数据进行变换。
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
$$tanh x = \frac{sinh x}{cosh x} = \frac{e^x-e^{-x}}{e^x+e^{-x}}$$

层类型：`TanH`
```
layer {
  name: "layer"
  bottom: "in"
  top: "out"
  type: "TanH"
}
```

4.`Absolute Value——绝对值`
求每个输入数据的绝对值。
```
f(x)=Abs(x)
```
层类型：`AbsVal`
```
layer {
  name: "layer"
  bottom: "in"
  top: "out"
  type: "AbsVal"
}
```

5.`Power——幂`
对每个输入数据进行`幂`运算
```
f(x)= (shift + scale * x) ^ power
```
层类型：`Power`
可选参数：
  `power`: 指数,默认为1
  `scale`: 比例,默认为1
  `shift`: 偏移,默认为0

```
layer {
  name: "layer"
  bottom: "in"
  top: "out"
  type: "Power"
  power_param {
    power: 2
    scale: 1
    shift: 0
  }
}
```

6.`BNLL`
`binomial normal log likelihood`的简称,二项正态对数似然。
```
f(x)=log(1 + exp(x))
```
层类型：`BNLL`
```
layer {
  name: "layer"
  bottom: "in"
  top: "out"
  type: “BNLL”
}
```