# 视觉层（Vision Layers)及参数

所有的层都具有的参数，如`name`, `type`, `bottom`, `top`和`transform_param`请参看我的前一篇文章:[Caffe学习系列(2)：数据层及参数](./3. data layer and paramters.md)。

本文只讲解视觉层`Vision Layers`的参数，视觉层包括`Convolution`, `Pooling`, `Local Response Normalization (LRN)`, `im2col`等层。

1.`Convolution`层
 就是卷积层，是卷积神经网络`CNN`的核心层。

1.1 层类型：`Convolution`
 (1)`lr_mult`: 学习率的系数，最终的学习率是这个数乘以solver.prototxt配置文件中的`base_lr`。如果有两个`lr_mult`, 则第一个表示`权值的学习率`，第二个表示`偏置项的学习率`。一般`偏置项的学习率`是`权值学习率`的`2`倍。
 (2) 紧随其后的`convolution_param`中，我们可以设定卷积层的特有参数。
  <1> 必须设置的参数：
  　`num_output`: 卷积核（filter)的个数
    `kernel_size`: 卷积核的大小。如果卷积核的长和宽不等，需要用`kernel_h`和`kernel_w`分别设定
  <2> 其它参数：
  　`stride`: 卷积核的步长，默认为1。也可以用`stride_h`和`stride_w`来设置。
　　`pad`:  扩充边缘，默认为0，不扩充。 
      扩充的时候是左右、上下对称的，比如卷积核的大小为`5*5`，那么`pad`设置为`2`，则四个边缘都扩充`2`个像素，即宽度和高度都扩充了`4`个像素,这样卷积运  算之后的特征图就不会变小。也可以通过`pad_h`和`pad_w`来分别设定。
 　 `weight_filler`: 权值初始化。 默认为`constant`,值全为`0`。
      很多时候我们用`xavier`算法来进行初始化，也可以设置为`gaussian`
    `bias_filler`: 偏置项的初始化。一般设置为`constant`,值全为`0`。
    `bias_term`: 是否开启偏置项，默认为`true`, 开启.
    `group`: 分组，默认为`1`组。
      如果大于`1`，我们限制卷积的连接操作在一个子集内。如果我们根据图像的通道来分组，那么第`i`个输出分组只能与第`i`个输入分组进行连接。

**示例`1`**
```
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
```

1.2 `Convolution`层的运算方法
输入：`n*c0*w0*h0`
输出：`n*c1*w1*h1`
其中，`c1`就是参数中的`num_output`，生成的特征图个数
```
 w1=(w0+2*pad-kernel_size)/stride+1;
 h1=(h0+2*pad-kernel_size)/stride+1;
```
如果设置`stride`为`1`，前后两次卷积部分存在重叠。如果设置`pad=(kernel_size-1)/2`,则运算后，宽度和高度不变。

>要想全面理解上述参数的含义，需要先搞清楚卷积的基本概念。

2.`Pooling层`
也叫`池化层`，为了`减少运算量`和`数据维度`而设置的一种层。

2.1 层类型：`Pooling`
  (1) 必须设置的参数：
    `kernel_size`: 池化的核大小。也可以用`kernel_h`和`kernel_w`分别设定。
  (2) 其它参数：
    `pool`: 池化方法，默认为`MAX`。目前可用的方法有`MAX`, `AVE`, 或`STOCHASTIC`
  　`pad`: 和卷积层的`pad`的一样，进行边缘扩充。默认为`0`
  　`stride`: 池化的步长，默认为`1`。一般我们设置为`2`，即不重叠。也可以用`stride_h`和`stride_w`来设置。

**示例`2`**：
```
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
```
2.2 `pooling`层的运算方法
  `pooling`层的运算方法基本是和卷积层是一样的。
输入：`n*c*w0*h0`
输出：`n*c*w1*h1`
和卷积层的区别就是其中的`c`保持不变
```
 w1=(w0+2*pad-kernel_size)/stride+1;
 h1=(h0+2*pad-kernel_size)/stride+1;
```
如果设置`stride`为`2`，前后两次卷积部分不重叠。`100*100`的特征图池化后，变成`50*50`.
 
3.`Local Response Normalization`(LRN)层
 此层是对一个输入的局部区域进行归一化，达到`侧抑制`的效果。可去搜索`AlexNet`或`GoogLenet`，里面就用到了这个功能。

3.1 层类型：LRN
参数：全部为可选，没有必须
　`local_size`: 默认为`5`。如果是跨通道LRN，则表示求和的通道数；如果是在通道内LRN，则表示求和的正方形区域长度。
  `alpha`: 默认为`1`，归一化公式中的参数。
　`beta`: 默认为`5`，归一化公式中的参数。
　`norm_region`: 默认为`ACROSS_CHANNELS`。有两个选择，`ACROSS_CHANNELS`表示在相邻的通道间求和归一化。`WITHIN_CHANNEL`表示在一个通道内部特定的区域内进行求和归一化。与前面的`local_size`参数对应。
 
归一化公式：对于每一个输入, 去除以<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
$$1 + (\alpha/n)(\Sigma _ i x^2_ i)^\beta$$
得到归一化后的输出。

 
**示例`3`**：
```
layers {
  name: "norm1"
  type: LRN
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
```

4.`im2col`层

如果对`matlab`比较熟悉的话，就应该知道`im2col`是什么意思。它先将一个大矩阵，重叠地划分为多个子矩阵，对每个子矩阵序列化成向量，最后得到另外一个矩阵。

看一看图就知道了：
![](http://images2015.cnblogs.com/blog/140867/201512/140867-20151224103800781-1819163043.png)

在`caffe`中，`卷积`运算就是先对数据进行`im2col`操作，再进行`内积`运算（inner product)。这样做，比原始的卷积操作速度更快。

看看两种卷积操作的异同：
![](http://images2015.cnblogs.com/blog/140867/201512/140867-20151224103937640-1245969956.jpg)
 