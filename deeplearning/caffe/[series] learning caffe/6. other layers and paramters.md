# 其它常用层及参数

本文讲解一些其它的常用层，包括：`softmax_loss`层，`Inner Product`层，`accuracy`层，`reshape`层和`dropout`层及其它们的参数配置。

1.`softmax-loss`

`softmax-loss`层和`softmax`层计算大致是相同的。`softmax`是一个分类器，计算的是类别的`概率`（Likelihood），是`Logistic Regression`的一种扩展。`Logistic Regression`只能用于二分类，而`softmax`可以用于多分类。

1.1 `softmax`与`softmax-loss`的区别：
(1) `softmax`计算公式：
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
$$P_ {j} = \frac{e^\alpha _ {j}}{\Sigma_ {k} e^{\alpha_ {k}}}$$

(2) `softmax-loss`计算公式：
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
$$L = - \sum_{k} y_ {j}\log p_ {j}$$


关于两者的区别更加具体的介绍，可参考：[softmax vs. softmax-loss](http://freemind.pluskid.org/machine-learning/softmax-vs-softmax-loss-numerical-stability/)

用户可能最终目的就是得到各个类别的`概率似然值`，这个时候就只需要一个 `Softmax`层，而不一定要进行`softmax-Loss`操作；或者是用户有通过其他方式已经得到了`某种概率似然值`，然后要做`最大似然估计`，此时则只需要后面的 `softmax-Loss` 而不需要前面的 `Softmax` 操作。因此提供两个不同的 `Layer` 结构比只提供一个合在一起的 `Softmax-Loss` Layer 要灵活许多。

不管是`softmax layer`还是`softmax-loss layer`,都是没有参数的，只是层类型不同而已。

(3) `softmax-loss` layer：输出`loss`值
```
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
```

(4) `softmax` layer: 输出`似然`值
```
layers {
  bottom: "cls3_fc"
  top: "prob"
  name: "prob"
  type: “Softmax"
}
```

2.`Inner Product`
`全连接层`，把输入当作成一个`向量`，输出也是一个简单`向量`（把输入数据`blobs`的`width`和`height`全变为1）。
输入：`n*c0*h*w`
输出：`n*c1*1*1`
全连接层实际上也是一种`卷积层`，只是它的`卷积核`大小和`原数据`大小一致。因此它的参数基本和卷积层的参数一样。

层类型：`InnerProduct`
 (1)`lr_mult`: 学习率的系数，最终的学习率是这个数乘以solver.prototxt配置文件中的base_lr。如果有两个lr_mult, 则第一个表示权值的学习率，第二个表示偏置项的学习率。一般偏置项的学习率是权值学习率的两倍。
 (2) 必须设置的参数：
　`num_output`: 过滤器（filfter)的个数
 (3) 其它参数：
　`weight_filler`: 权值初始化。 默认为`constant`,值全为0，很多时候我们用`xavier`算法来进行初始化，也可以设置为`gaussian`
  `bias_filler`: 偏置项的初始化。一般设置为"constant",值全为0。
  `bias_term`: 是否开启偏置项，默认为true, 开启。

```
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
```

3.`accuracy`
输出分类（预测）精确度，只有`test`阶段才有，因此需要加入`include`参数。

层类型：`Accuracy`
```
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
```

4.`reshape`

在不改变数据的情况下，改变输入的维度。

层类型：`Reshape`
```
 layer {
    name: "reshape"
    type: "Reshape"
    bottom: "input"
    top: "output"
    reshape_param {
      shape {
        dim: 0  # copy the dimension from below
        dim: 2
        dim: 3
        dim: -1 # infer it from the other dimensions
      }
    }
  }
```
有一个可选的参数组`shape`, 用于指定`blob`数据的各维的值（`blob`是一个四维的数据：`n*c*w*h`）。
`dim:0`  表示维度不变，即输入和输出是相同的维度。
`dim:2`  或 dim:3 将原来的维度变成2或3
`dim:-1` 表示由系统自动计算维度。数据的总量不变，系统会根据blob数据的其它三维来自动计算当前维的维度值 。

假设原数据为：`64*3*28*28`， 表示`64张3通道的28*28`的彩色图片

经过reshape变换：
```
   reshape_param {
      shape {
        dim: 0 
        dim: 0
        dim: 14
        dim: -1 
      }
    }
```
输出数据为：64*3*14*56

5.`Dropout`
 `Dropout`是一个防止`过拟合(overfit)`的trick。可以随机让网络某些隐含层节点的权重不工作。

```
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7-conv"
  top: "fc7-conv"
  dropout_param {
    dropout_ratio: 0.5
  }
}
```
只需要设置一个`dropout_ratio`就可以了。
还有其它更多的层，但用的地方不多，就不一一介绍了。随着深度学习的深入，各种各样的新模型会不断的出现，因此对应的各种新类型的层也在不断的出现。这些新出现的层，我们只有在等caffe更新到新版本后，再去慢慢地摸索了。