# 全连接层的作用是什么？
<知乎问答>

在卷积神经网络尚未火热的年代，人们使用`haar/lbp + adaboost`级连的组合方式检测人脸，hog+svm的组合方式检测行人。这种传统的目标检测方法一个认知上的优势就是: 模块的功能明确，划分得很清晰，符合人们的理解方式。其中，haar，lbp，hog等手工设计的特征提取算子用于提取特征，adaboost，svm用于对提取的特征分类。  
而早期的全连接神经网络，就是属于用于对提取的特征进行分类的模块，我们也可以同样将神经网络替换掉adaboost，svm用于分类。  
后来将神经网络用于图像分类中，面对早期的小分辨率图片，我们依旧可以使用特征提取+神经网络分类的模式，也可以直接将每个像素点作为特征输入用于神经网络的分类。但面对后期的大分辨率图像，如果依旧使用逐像素点输入的方式，很明显将会导致全连接中的权值过于稀疏，造成模型训练的困难。  
而卷积神经网络中conv+relu(早期为sigmoid)+pooling(以下称三剑客)的组合，不仅可以替代手工设计特征算子的繁琐，而且局部感受野+权值共享的设计思想也能避免全连接网络中的种种弊端。此时人们将三剑客的组合视为特征提取的过程，如果按照早期人们特征提取+分类的设计思路，那么分类使用全连接的设计方式，就可以刚好实现了一个end-to-end的架构，也即早起卷积神经网络的原型。  
但必须明白的是，虽然模型完成了一个end-to-end的设计架构，可以直接用于训练和分类，但在人们的认知上，特征提取和分类依然是分开的，也就是说三剑客用于特征提取，全连接用于分类。   
后来随着更优秀分类网络的出现(alexnet，vgg等)，人们不再仅仅满足于分类准确率的提升，面对动辄两三百M的模型，人们思考能否减少模型的大小。人们通过研究发现，在包含全连接的网络中，全连接的参数占据了所有参数中的大部分比例，这样使得精简全连接参数变得刻不容缓。   
于是一部分优秀的人们想到了使用svd等方式减少参数，另一部分优秀的人们开始思考: 是否真的需要使用全连接层，或者有无可以替代全连接层的方法？   
于是就出现了如nin，squeezenet中，直接使用global average pooling的方式，直接替代全连接层。人们发现不使用全连接层，模型的检准率并没有降低，而模型的大小却极大的减少(当然了，也包括以上网络中其他模块优化的功劳，如`1*1`卷积的使用等)。   
另一方面，同样在nin，以及用于图像分类的fcn中，人们发现使用`1*1`卷积，也可以达到与全连接层同样的功效，依然能保证同样的检准率(`1*1`卷积的使用，对比fc并不能 减少模型参数，特此说明)。  
这时候人们就又开始重新思考，全连接层在卷积神经网络中真正的作用是什么了。于是就又有了`@魏秀参`回答中新的探索。   
最后总结就是，卷积神经网络中全连接层的设计，属于人们在传统特征提取+分类思维下的一种"迁移学习"思想，但在这种end-to-end的模型中，其用于分类的功能其实是被弱化了，而全连接层参数过多的缺点也激励着人们设计出更好的模型替代之达到更好的效果。同时，也将促进我们更深入地探讨其中的奥秘。  

## 评论区
“人们发现使用`1*1`卷积，也可以达到与全连接层同样的功效，在保证检准率的同时，依旧可以压缩模型参数”，这句话似乎不太准确。用`1*1` convolution来模拟达到fully connected的效果，使用到参数应该是一样多的。PS: 似乎LeCun喜欢`1*1` convolution这种说法，说根本没有fully connected这回事。   

## @魏秀参
南京大学 计算机科学与技术博士在读
全连接层到底什么用？我来谈三点。   
全连接层（fully connected layers，FC）在整个卷积神经网络中起到“分类器”的作用。如果说卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。在实际使用中，全连接层可由卷积操作实现：对前层是全连接的全连接层可以转化为卷积核为1x1的卷积；而前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽（注1）。   
目前由于全连接层参数冗余（仅全连接层参数就可占整个网络参数80%左右），近期一些性能优异的网络模型如ResNet和GoogLeNet等均用全局平均池化（global average pooling，GAP）取代FC来融合学到的深度特征，最后仍用softmax等损失函数作为网络目标函数来指导学习过程。需要指出的是，用GAP替代FC的网络通常有较好的预测性能。具体案例可参见我们在ECCV'16（视频）表象性格分析竞赛中获得冠军的做法：「冠军之道」Apparent Personality Analysis竞赛经验分享 - 知乎专栏 ，project：Deep Bimodal Regression for Apparent Personality Analysis    
在FC越来越不被看好的当下，我们近期的研究（In Defense of Fully Connected Layers in Visual Representation Transfer）发现，FC可在模型表示能力迁移过程中充当“防火墙”的作用。具体来讲，假设在ImageNet上预训练得到的模型为\mathcal{M} ，则ImageNet可视为源域（迁移学习中的source domain）。微调（fine tuning）是深度学习领域最常用的迁移学习技术。针对微调，若目标域（target domain）中的图像与源域中图像差异巨大（如相比ImageNet，目标域图像不是物体为中心的图像，而是风景照，见下图），不含FC的网络微调后的结果要差于含FC的网络。因此FC可视作模型表示能力的“防火墙”，特别是在源域与目标域差异较大的情况下，FC可保持较大的模型capacity从而保证模型表示能力的迁移。（冗余的参数并不一无是处。）    
注1: 有关卷积操作“实现”全连接层，有必要多啰嗦几句。    
以VGG-16为例，对224x224x3的输入，最后一层卷积可得输出为7x7x512，如后层是一层含4096个神经元的FC，则可用卷积核为7x7x512x4096的全局卷积来实现这一全连接运算过程，其中该卷积核参数如下：   
“filter size = 7, padding = 0, stride = 1, D_in = 512, D_out = 4096”   
经过此卷积操作后可得输出为1x1x4096。  
如需再次叠加一个2048的FC，则可设定参数为“filter size = 1, padding = 0, stride = 1, D_in = 4096, D_out = 2048”的卷积层操作。   

